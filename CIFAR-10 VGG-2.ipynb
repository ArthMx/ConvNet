{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG-like model for the CIFAR-10 dataset\n",
    "\n",
    "In this notebook, I present a model able to reach 90 % accuracy on test set by using data augmentation and using an architecture inspired by the VGG model.\n",
    "\n",
    "- Karen Simonyan, Andrew Zisserman, 2015, Very Deep Convolutional Networks for Large-Scale Image Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthur/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape : (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('X shape :', x_train.shape)\n",
    "print(len(x_train), 'train samples')\n",
    "print(len(x_test), 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_mean = np.mean(x_train, axis=0)\n",
    "x_train_std = np.std(x_train, axis=0)\n",
    "\n",
    "x_train = (x_train - x_train_mean)/x_train_std\n",
    "x_test = (x_test - x_train_mean)/x_train_std\n",
    "\n",
    "n_y = 10\n",
    "y_train = keras.utils.to_categorical(y_train, n_y)\n",
    "y_test = keras.utils.to_categorical(y_test, n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 2, 2, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 2, 2, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 1,218,378\n",
      "Trainable params: 1,216,074\n",
      "Non-trainable params: 2,304\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "activation = 'relu'\n",
    "n_l0 = 64\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(n_l0, (3,3), padding='same', activation=activation, input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(n_l0, (3,3), padding='same', activation=activation))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2,2), strides=2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(n_l0*2, (3,3), padding='valid', activation=activation))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(n_l0*2, (3,3), padding='valid', activation=activation))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2,2), strides=2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(n_l0*4, (3,3), padding='valid', activation=activation))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(n_l0*4, (3,3), padding='valid', activation=activation))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2,2), strides=2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=activation))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_y, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "781/781 [==============================] - 27s 34ms/step - loss: 1.9706 - acc: 0.3299 - val_loss: 1.4087 - val_acc: 0.4941\n",
      "Epoch 2/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 1.3726 - acc: 0.5030 - val_loss: 1.2141 - val_acc: 0.5902\n",
      "Epoch 3/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 1.1535 - acc: 0.5913 - val_loss: 1.2079 - val_acc: 0.6051\n",
      "Epoch 4/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 1.0149 - acc: 0.6446 - val_loss: 0.8662 - val_acc: 0.7027\n",
      "Epoch 5/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.9213 - acc: 0.6799 - val_loss: 0.8226 - val_acc: 0.7155\n",
      "Epoch 6/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.8634 - acc: 0.7013 - val_loss: 0.7509 - val_acc: 0.7426\n",
      "Epoch 7/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.8160 - acc: 0.7204 - val_loss: 0.7602 - val_acc: 0.7392\n",
      "Epoch 8/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.7746 - acc: 0.7312 - val_loss: 0.6415 - val_acc: 0.7770\n",
      "Epoch 9/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.7409 - acc: 0.7481 - val_loss: 0.5898 - val_acc: 0.7980\n",
      "Epoch 10/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.7158 - acc: 0.7544 - val_loss: 0.6492 - val_acc: 0.7823\n",
      "Epoch 11/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.6885 - acc: 0.7645 - val_loss: 0.6091 - val_acc: 0.7938\n",
      "Epoch 12/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.6649 - acc: 0.7741 - val_loss: 0.5691 - val_acc: 0.8059\n",
      "Epoch 13/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.6516 - acc: 0.7777 - val_loss: 0.5925 - val_acc: 0.7973\n",
      "Epoch 14/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.6331 - acc: 0.7841 - val_loss: 0.5363 - val_acc: 0.8160\n",
      "Epoch 15/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.6202 - acc: 0.7896 - val_loss: 0.5724 - val_acc: 0.8063\n",
      "Epoch 16/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.6048 - acc: 0.7936 - val_loss: 0.5453 - val_acc: 0.8160\n",
      "Epoch 17/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.5916 - acc: 0.7984 - val_loss: 0.4885 - val_acc: 0.8323\n",
      "Epoch 18/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.5823 - acc: 0.8022 - val_loss: 0.5406 - val_acc: 0.8180\n",
      "Epoch 19/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.5663 - acc: 0.8091 - val_loss: 0.5181 - val_acc: 0.8250\n",
      "Epoch 20/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.5644 - acc: 0.8080 - val_loss: 0.4996 - val_acc: 0.8314\n",
      "Epoch 21/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.5541 - acc: 0.8122 - val_loss: 0.5272 - val_acc: 0.8255\n",
      "Epoch 22/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.5416 - acc: 0.8157 - val_loss: 0.4693 - val_acc: 0.8401\n",
      "Epoch 23/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.5325 - acc: 0.8182 - val_loss: 0.4851 - val_acc: 0.8358\n",
      "Epoch 24/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.5275 - acc: 0.8210 - val_loss: 0.4964 - val_acc: 0.8320\n",
      "Epoch 25/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.5217 - acc: 0.8228 - val_loss: 0.4918 - val_acc: 0.8368\n",
      "Epoch 26/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.5121 - acc: 0.8262 - val_loss: 0.4671 - val_acc: 0.8458\n",
      "Epoch 27/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.5077 - acc: 0.8268 - val_loss: 0.4189 - val_acc: 0.8610\n",
      "Epoch 28/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.5028 - acc: 0.8286 - val_loss: 0.4583 - val_acc: 0.8427\n",
      "Epoch 29/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.4942 - acc: 0.8326 - val_loss: 0.4417 - val_acc: 0.8539\n",
      "Epoch 30/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.4882 - acc: 0.8352 - val_loss: 0.4450 - val_acc: 0.8523\n",
      "Epoch 31/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.4846 - acc: 0.8375 - val_loss: 0.4239 - val_acc: 0.8571\n",
      "Epoch 32/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.4859 - acc: 0.8349 - val_loss: 0.4313 - val_acc: 0.8557\n",
      "Epoch 33/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.4713 - acc: 0.8409 - val_loss: 0.4770 - val_acc: 0.8390\n",
      "Epoch 34/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.4662 - acc: 0.8427 - val_loss: 0.4174 - val_acc: 0.8607\n",
      "Epoch 35/200\n",
      "781/781 [==============================] - 25s 31ms/step - loss: 0.4653 - acc: 0.8412 - val_loss: 0.3961 - val_acc: 0.8637\n",
      "Epoch 36/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.4604 - acc: 0.8427 - val_loss: 0.4301 - val_acc: 0.8546\n",
      "Epoch 37/200\n",
      "781/781 [==============================] - 25s 31ms/step - loss: 0.4511 - acc: 0.8463 - val_loss: 0.4116 - val_acc: 0.8643\n",
      "Epoch 38/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.4522 - acc: 0.8466 - val_loss: 0.4123 - val_acc: 0.8621\n",
      "Epoch 39/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.4468 - acc: 0.8478 - val_loss: 0.4115 - val_acc: 0.8590\n",
      "Epoch 40/200\n",
      "781/781 [==============================] - 25s 31ms/step - loss: 0.4452 - acc: 0.8494 - val_loss: 0.4521 - val_acc: 0.8512\n",
      "Epoch 41/200\n",
      "781/781 [==============================] - 25s 31ms/step - loss: 0.4431 - acc: 0.8495 - val_loss: 0.3689 - val_acc: 0.8755\n",
      "Epoch 42/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.4394 - acc: 0.8505 - val_loss: 0.4431 - val_acc: 0.8546\n",
      "Epoch 43/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.4302 - acc: 0.8528 - val_loss: 0.4117 - val_acc: 0.8600\n",
      "Epoch 44/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.4291 - acc: 0.8537 - val_loss: 0.3818 - val_acc: 0.8726\n",
      "Epoch 45/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.4323 - acc: 0.8530 - val_loss: 0.3814 - val_acc: 0.8717\n",
      "Epoch 46/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.4264 - acc: 0.8563 - val_loss: 0.4168 - val_acc: 0.8638\n",
      "Epoch 47/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.4214 - acc: 0.8570 - val_loss: 0.4039 - val_acc: 0.8648\n",
      "Epoch 48/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.4236 - acc: 0.8566 - val_loss: 0.4125 - val_acc: 0.8634\n",
      "Epoch 49/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.4160 - acc: 0.8575 - val_loss: 0.4019 - val_acc: 0.8642\n",
      "Epoch 50/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.4178 - acc: 0.8577 - val_loss: 0.4012 - val_acc: 0.8622\n",
      "Epoch 51/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.4083 - acc: 0.8620 - val_loss: 0.4136 - val_acc: 0.8589\n",
      "Epoch 52/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.4045 - acc: 0.8610 - val_loss: 0.3933 - val_acc: 0.8706\n",
      "Epoch 53/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.4037 - acc: 0.8627 - val_loss: 0.4220 - val_acc: 0.8602\n",
      "Epoch 54/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.4049 - acc: 0.8631 - val_loss: 0.3767 - val_acc: 0.8752\n",
      "Epoch 55/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.4040 - acc: 0.8622 - val_loss: 0.3734 - val_acc: 0.8709\n",
      "Epoch 56/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.3965 - acc: 0.8648 - val_loss: 0.3758 - val_acc: 0.8752\n",
      "Epoch 57/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.3932 - acc: 0.8662 - val_loss: 0.3979 - val_acc: 0.8673\n",
      "Epoch 58/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.3989 - acc: 0.8637 - val_loss: 0.3926 - val_acc: 0.8707\n",
      "Epoch 59/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.3891 - acc: 0.8664 - val_loss: 0.3544 - val_acc: 0.8837\n",
      "Epoch 60/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.3924 - acc: 0.8668 - val_loss: 0.3709 - val_acc: 0.8754\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 24s 30ms/step - loss: 0.3875 - acc: 0.8690 - val_loss: 0.3994 - val_acc: 0.8655\n",
      "Epoch 62/200\n",
      "781/781 [==============================] - 24s 30ms/step - loss: 0.3883 - acc: 0.8674 - val_loss: 0.3789 - val_acc: 0.8733\n",
      "Epoch 63/200\n",
      "781/781 [==============================] - 24s 30ms/step - loss: 0.3813 - acc: 0.8702 - val_loss: 0.3918 - val_acc: 0.8693\n",
      "Epoch 64/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3766 - acc: 0.8714 - val_loss: 0.3692 - val_acc: 0.8762\n",
      "Epoch 65/200\n",
      "781/781 [==============================] - 24s 30ms/step - loss: 0.3786 - acc: 0.8703 - val_loss: 0.3623 - val_acc: 0.8755\n",
      "Epoch 66/200\n",
      "781/781 [==============================] - 24s 30ms/step - loss: 0.3759 - acc: 0.8715 - val_loss: 0.4113 - val_acc: 0.8660\n",
      "Epoch 67/200\n",
      "781/781 [==============================] - 24s 30ms/step - loss: 0.3751 - acc: 0.8712 - val_loss: 0.3520 - val_acc: 0.8824\n",
      "Epoch 68/200\n",
      "781/781 [==============================] - 24s 30ms/step - loss: 0.3679 - acc: 0.8737 - val_loss: 0.3721 - val_acc: 0.8762\n",
      "Epoch 69/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3726 - acc: 0.8737 - val_loss: 0.3762 - val_acc: 0.8761\n",
      "Epoch 70/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3698 - acc: 0.8740 - val_loss: 0.3640 - val_acc: 0.8790\n",
      "Epoch 71/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3696 - acc: 0.8727 - val_loss: 0.3964 - val_acc: 0.8688\n",
      "Epoch 72/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3696 - acc: 0.8733 - val_loss: 0.3769 - val_acc: 0.8771\n",
      "Epoch 73/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3688 - acc: 0.8741 - val_loss: 0.3704 - val_acc: 0.8768\n",
      "Epoch 74/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3610 - acc: 0.8763 - val_loss: 0.3889 - val_acc: 0.8732\n",
      "Epoch 75/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3643 - acc: 0.8745 - val_loss: 0.3577 - val_acc: 0.8789\n",
      "Epoch 76/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3583 - acc: 0.8779 - val_loss: 0.3668 - val_acc: 0.8799\n",
      "Epoch 77/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3599 - acc: 0.8777 - val_loss: 0.3631 - val_acc: 0.8787\n",
      "Epoch 78/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3549 - acc: 0.8782 - val_loss: 0.3961 - val_acc: 0.8714\n",
      "Epoch 79/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3584 - acc: 0.8776 - val_loss: 0.3618 - val_acc: 0.8810\n",
      "Epoch 80/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3510 - acc: 0.8803 - val_loss: 0.3688 - val_acc: 0.8769\n",
      "Epoch 81/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3523 - acc: 0.8792 - val_loss: 0.4144 - val_acc: 0.8666\n",
      "Epoch 82/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3567 - acc: 0.8775 - val_loss: 0.3312 - val_acc: 0.8878\n",
      "Epoch 83/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3468 - acc: 0.8814 - val_loss: 0.3458 - val_acc: 0.8828\n",
      "Epoch 84/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3504 - acc: 0.8799 - val_loss: 0.3581 - val_acc: 0.8812\n",
      "Epoch 85/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3495 - acc: 0.8803 - val_loss: 0.3449 - val_acc: 0.8853\n",
      "Epoch 86/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3457 - acc: 0.8833 - val_loss: 0.3495 - val_acc: 0.8829\n",
      "Epoch 87/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3469 - acc: 0.8808 - val_loss: 0.3522 - val_acc: 0.8834\n",
      "Epoch 88/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3394 - acc: 0.8837 - val_loss: 0.3619 - val_acc: 0.8817\n",
      "Epoch 89/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3416 - acc: 0.8823 - val_loss: 0.3446 - val_acc: 0.8882\n",
      "Epoch 90/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3445 - acc: 0.8823 - val_loss: 0.3553 - val_acc: 0.8835\n",
      "Epoch 91/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3452 - acc: 0.8817 - val_loss: 0.3561 - val_acc: 0.8849\n",
      "Epoch 92/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3441 - acc: 0.8794 - val_loss: 0.3308 - val_acc: 0.8925\n",
      "Epoch 93/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3388 - acc: 0.8851 - val_loss: 0.3605 - val_acc: 0.8842\n",
      "Epoch 94/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3369 - acc: 0.8845 - val_loss: 0.3772 - val_acc: 0.8771\n",
      "Epoch 95/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3298 - acc: 0.8867 - val_loss: 0.3762 - val_acc: 0.8754\n",
      "Epoch 96/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3364 - acc: 0.8841 - val_loss: 0.3594 - val_acc: 0.8793\n",
      "Epoch 97/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3318 - acc: 0.8876 - val_loss: 0.3592 - val_acc: 0.8835\n",
      "Epoch 98/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3255 - acc: 0.8874 - val_loss: 0.3725 - val_acc: 0.8818\n",
      "Epoch 99/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3411 - acc: 0.8821 - val_loss: 0.3742 - val_acc: 0.8779\n",
      "Epoch 100/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3274 - acc: 0.8872 - val_loss: 0.3878 - val_acc: 0.8748\n",
      "Epoch 101/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3280 - acc: 0.8870 - val_loss: 0.3618 - val_acc: 0.8830\n",
      "Epoch 102/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3249 - acc: 0.8892 - val_loss: 0.3471 - val_acc: 0.8828\n",
      "Epoch 103/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3259 - acc: 0.8897 - val_loss: 0.3865 - val_acc: 0.8754\n",
      "Epoch 104/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3273 - acc: 0.8866 - val_loss: 0.3522 - val_acc: 0.8827\n",
      "Epoch 105/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3272 - acc: 0.8893 - val_loss: 0.3549 - val_acc: 0.8828\n",
      "Epoch 106/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3223 - acc: 0.8906 - val_loss: 0.4105 - val_acc: 0.8671\n",
      "Epoch 107/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3263 - acc: 0.8882 - val_loss: 0.4008 - val_acc: 0.8723\n",
      "Epoch 108/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3252 - acc: 0.8899 - val_loss: 0.3823 - val_acc: 0.8786\n",
      "Epoch 109/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.3197 - acc: 0.8902 - val_loss: 0.3644 - val_acc: 0.8809\n",
      "Epoch 110/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.3176 - acc: 0.8895 - val_loss: 0.3520 - val_acc: 0.8888\n",
      "Epoch 111/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.3180 - acc: 0.8908 - val_loss: 0.3671 - val_acc: 0.8807\n",
      "Epoch 112/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.3209 - acc: 0.8899 - val_loss: 0.3650 - val_acc: 0.8795\n",
      "Epoch 00112: early stopping\n",
      "Total training time : 2730.980 s\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.adam(lr=0.001)\n",
    "model.compile(optimizer, keras.losses.categorical_crossentropy, ['accuracy'])\n",
    "\n",
    "shift = 4/32\n",
    "generator = ImageDataGenerator(rotation_range=10, width_shift_range=shift, height_shift_range=shift, \n",
    "                               horizontal_flip=True)\n",
    "\n",
    "batch_size = 64\n",
    "n_steps = x_train.shape[0]//batch_size\n",
    "\n",
    "save_path = './Model_trained/VGG_2_model_cifar10.h5'\n",
    "ckeckpoint = ModelCheckpoint(save_path, save_best_only=True)\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit_generator(generator.flow(x_train, y_train, batch_size=batch_size), steps_per_epoch=n_steps, \n",
    "                    epochs=200, validation_data=(x_test, y_test), callbacks=[ckeckpoint, early_stopping])\n",
    "print('Total training time : %.3f s' %(time.time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 215us/step\n",
      "Best model test accuracy : 0.8925\n"
     ]
    }
   ],
   "source": [
    "best_model = keras.models.load_model(save_path)\n",
    "print('Best model test accuracy :', best_model.evaluate(x_test, y_test, batch_size=64)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG-like model for the CIFAR-10 dataset\n",
    "\n",
    "In this notebook, I present a model able to reach 90 % accuracy on test set by using data augmentation and using an architecture inspired by the VGG model.\n",
    "\n",
    "- Karen Simonyan, Andrew Zisserman, 2015, Very Deep Convolutional Networks for Large-Scale Image Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthur/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape : (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('X shape :', x_train.shape)\n",
    "print(len(x_train), 'train samples')\n",
    "print(len(x_test), 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_mean = np.mean(x_train, axis=0)\n",
    "x_train_std = np.std(x_train, axis=0)\n",
    "\n",
    "x_train = (x_train - x_train_mean)/x_train_std\n",
    "x_test = (x_test - x_train_mean)/x_train_std\n",
    "\n",
    "n_y = 10\n",
    "y_train = keras.utils.to_categorical(y_train, n_y)\n",
    "y_test = keras.utils.to_categorical(y_test, n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 2,240,042\n",
      "Trainable params: 2,236,074\n",
      "Non-trainable params: 3,968\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "activation = 'relu'\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3,3), padding='same', input_shape=input_shape, activation=activation))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3,3), padding='same', activation=activation))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2,2), strides=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding='same', activation=activation))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3,3), padding='same', activation=activation))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2,2), strides=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), padding='same', activation=activation))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3,3), padding='same', activation=activation))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2,2), strides=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256, (3,3), padding='same', activation=activation))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3,3), padding='same', activation=activation))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2,2), strides=2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation=activation))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_y, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 1.9483 - acc: 0.3697 - val_loss: 1.5767 - val_acc: 0.4889\n",
      "Epoch 2/200\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 1.3977 - acc: 0.5235 - val_loss: 2.9576 - val_acc: 0.4749\n",
      "Epoch 3/200\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 1.2107 - acc: 0.5916 - val_loss: 0.9561 - val_acc: 0.6682\n",
      "Epoch 4/200\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 1.0178 - acc: 0.6539 - val_loss: 1.1099 - val_acc: 0.6664\n",
      "Epoch 5/200\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.9177 - acc: 0.6888 - val_loss: 0.8687 - val_acc: 0.7195\n",
      "Epoch 6/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.8503 - acc: 0.7113 - val_loss: 0.8723 - val_acc: 0.7242\n",
      "Epoch 7/200\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.7850 - acc: 0.7317 - val_loss: 0.7582 - val_acc: 0.7592\n",
      "Epoch 8/200\n",
      "781/781 [==============================] - 21s 26ms/step - loss: 0.7455 - acc: 0.7471 - val_loss: 0.7121 - val_acc: 0.7802\n",
      "Epoch 9/200\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.7091 - acc: 0.7580 - val_loss: 0.6638 - val_acc: 0.7859\n",
      "Epoch 10/200\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6745 - acc: 0.7697 - val_loss: 0.6762 - val_acc: 0.7845\n",
      "Epoch 11/200\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6414 - acc: 0.7827 - val_loss: 0.6125 - val_acc: 0.8001\n",
      "Epoch 12/200\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6149 - acc: 0.7895 - val_loss: 0.5332 - val_acc: 0.8222\n",
      "Epoch 13/200\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.6009 - acc: 0.7947 - val_loss: 0.5450 - val_acc: 0.8156\n",
      "Epoch 14/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.5745 - acc: 0.8056 - val_loss: 0.5454 - val_acc: 0.8199\n",
      "Epoch 15/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.5615 - acc: 0.8101 - val_loss: 0.6137 - val_acc: 0.8063\n",
      "Epoch 16/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.5503 - acc: 0.8132 - val_loss: 0.4985 - val_acc: 0.8391\n",
      "Epoch 17/200\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5298 - acc: 0.8213 - val_loss: 0.5040 - val_acc: 0.8274\n",
      "Epoch 18/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.5117 - acc: 0.8240 - val_loss: 0.4881 - val_acc: 0.8359\n",
      "Epoch 19/200\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5008 - acc: 0.8282 - val_loss: 0.5336 - val_acc: 0.8358\n",
      "Epoch 20/200\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4959 - acc: 0.8293 - val_loss: 0.4761 - val_acc: 0.8430\n",
      "Epoch 21/200\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4797 - acc: 0.8355 - val_loss: 0.4458 - val_acc: 0.8498\n",
      "Epoch 22/200\n",
      "781/781 [==============================] - 21s 26ms/step - loss: 0.4704 - acc: 0.8392 - val_loss: 0.4805 - val_acc: 0.8410\n",
      "Epoch 23/200\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.4692 - acc: 0.8382 - val_loss: 0.5245 - val_acc: 0.8381\n",
      "Epoch 24/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.4562 - acc: 0.8436 - val_loss: 0.4585 - val_acc: 0.8466\n",
      "Epoch 25/200\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.4413 - acc: 0.8472 - val_loss: 0.4088 - val_acc: 0.8619\n",
      "Epoch 26/200\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.4352 - acc: 0.8506 - val_loss: 0.4383 - val_acc: 0.8573\n",
      "Epoch 27/200\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.4245 - acc: 0.8545 - val_loss: 0.3927 - val_acc: 0.8666\n",
      "Epoch 28/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.4217 - acc: 0.8554 - val_loss: 0.4129 - val_acc: 0.8619\n",
      "Epoch 29/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.4155 - acc: 0.8589 - val_loss: 0.4422 - val_acc: 0.8577\n",
      "Epoch 30/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.4078 - acc: 0.8599 - val_loss: 0.4078 - val_acc: 0.8666\n",
      "Epoch 31/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.4005 - acc: 0.8632 - val_loss: 0.3916 - val_acc: 0.8733\n",
      "Epoch 32/200\n",
      "781/781 [==============================] - 22s 29ms/step - loss: 0.3975 - acc: 0.8642 - val_loss: 0.3773 - val_acc: 0.8705\n",
      "Epoch 33/200\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.3948 - acc: 0.8657 - val_loss: 0.4164 - val_acc: 0.8633\n",
      "Epoch 34/200\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.3897 - acc: 0.8658 - val_loss: 0.3988 - val_acc: 0.8652\n",
      "Epoch 35/200\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.3856 - acc: 0.8685 - val_loss: 0.3922 - val_acc: 0.8718\n",
      "Epoch 36/200\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.3712 - acc: 0.8713 - val_loss: 0.3772 - val_acc: 0.8743\n",
      "Epoch 37/200\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.3695 - acc: 0.8718 - val_loss: 0.3472 - val_acc: 0.8834\n",
      "Epoch 38/200\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.3689 - acc: 0.8712 - val_loss: 0.4081 - val_acc: 0.8713\n",
      "Epoch 39/200\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.3595 - acc: 0.8758 - val_loss: 0.3721 - val_acc: 0.8766\n",
      "Epoch 40/200\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.3612 - acc: 0.8749 - val_loss: 0.3931 - val_acc: 0.8699\n",
      "Epoch 41/200\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.3563 - acc: 0.8782 - val_loss: 0.3548 - val_acc: 0.8793\n",
      "Epoch 42/200\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.3516 - acc: 0.8785 - val_loss: 0.4186 - val_acc: 0.8592\n",
      "Epoch 43/200\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.3445 - acc: 0.8809 - val_loss: 0.3593 - val_acc: 0.8811\n",
      "Epoch 44/200\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.3426 - acc: 0.8818 - val_loss: 0.3565 - val_acc: 0.8800\n",
      "Epoch 45/200\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.3410 - acc: 0.8828 - val_loss: 0.3740 - val_acc: 0.8769\n",
      "Epoch 46/200\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.3408 - acc: 0.8820 - val_loss: 0.3332 - val_acc: 0.8895\n",
      "Epoch 47/200\n",
      "781/781 [==============================] - 21s 28ms/step - loss: 0.3340 - acc: 0.8846 - val_loss: 0.4827 - val_acc: 0.8462\n",
      "Epoch 48/200\n",
      "781/781 [==============================] - 21s 26ms/step - loss: 0.3331 - acc: 0.8858 - val_loss: 0.3885 - val_acc: 0.8713\n",
      "Epoch 49/200\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.3231 - acc: 0.8879 - val_loss: 0.4035 - val_acc: 0.8694\n",
      "Epoch 50/200\n",
      "781/781 [==============================] - 21s 26ms/step - loss: 0.3248 - acc: 0.8875 - val_loss: 0.3561 - val_acc: 0.8821\n",
      "Epoch 51/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.3269 - acc: 0.8860 - val_loss: 0.3428 - val_acc: 0.8873\n",
      "Epoch 52/200\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.3181 - acc: 0.8910 - val_loss: 0.3341 - val_acc: 0.8865\n",
      "Epoch 53/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.3187 - acc: 0.8889 - val_loss: 0.4020 - val_acc: 0.8700\n",
      "Epoch 54/200\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.3172 - acc: 0.8912 - val_loss: 0.3868 - val_acc: 0.8758\n",
      "Epoch 55/200\n",
      "781/781 [==============================] - 23s 30ms/step - loss: 0.3125 - acc: 0.8919 - val_loss: 0.3265 - val_acc: 0.8899\n",
      "Epoch 56/200\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.3074 - acc: 0.8929 - val_loss: 0.3800 - val_acc: 0.8787\n",
      "Epoch 57/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.3031 - acc: 0.8937 - val_loss: 0.3651 - val_acc: 0.8815\n",
      "Epoch 58/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.3067 - acc: 0.8944 - val_loss: 0.3310 - val_acc: 0.8949\n",
      "Epoch 59/200\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.3041 - acc: 0.8956 - val_loss: 0.3760 - val_acc: 0.8809\n",
      "Epoch 60/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.2957 - acc: 0.8972 - val_loss: 0.3372 - val_acc: 0.8934\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 20s 26ms/step - loss: 0.2947 - acc: 0.8972 - val_loss: 0.3444 - val_acc: 0.8886\n",
      "Epoch 62/200\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.2923 - acc: 0.8981 - val_loss: 0.4017 - val_acc: 0.8725\n",
      "Epoch 63/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.2932 - acc: 0.8964 - val_loss: 0.3863 - val_acc: 0.8779\n",
      "Epoch 64/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.2883 - acc: 0.8994 - val_loss: 0.3275 - val_acc: 0.8940\n",
      "Epoch 65/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.2899 - acc: 0.9002 - val_loss: 0.3516 - val_acc: 0.8855\n",
      "Epoch 66/200\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.2847 - acc: 0.9017 - val_loss: 0.3454 - val_acc: 0.8893\n",
      "Epoch 67/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.2847 - acc: 0.9026 - val_loss: 0.3595 - val_acc: 0.8845\n",
      "Epoch 68/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.2819 - acc: 0.9010 - val_loss: 0.3303 - val_acc: 0.8943\n",
      "Epoch 69/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.2763 - acc: 0.9041 - val_loss: 0.3254 - val_acc: 0.8940\n",
      "Epoch 70/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.2739 - acc: 0.9045 - val_loss: 0.3417 - val_acc: 0.8895\n",
      "Epoch 71/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.2802 - acc: 0.9018 - val_loss: 0.3573 - val_acc: 0.8855\n",
      "Epoch 72/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.2722 - acc: 0.9049 - val_loss: 0.3362 - val_acc: 0.8892\n",
      "Epoch 73/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.2730 - acc: 0.9037 - val_loss: 0.3312 - val_acc: 0.8927\n",
      "Epoch 74/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.2747 - acc: 0.9043 - val_loss: 0.3326 - val_acc: 0.8940\n",
      "Epoch 75/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.2744 - acc: 0.9058 - val_loss: 0.3466 - val_acc: 0.8874\n",
      "Epoch 76/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.2667 - acc: 0.9073 - val_loss: 0.3203 - val_acc: 0.8935\n",
      "Epoch 77/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.2657 - acc: 0.9078 - val_loss: 0.3138 - val_acc: 0.8964\n",
      "Epoch 78/200\n",
      "781/781 [==============================] - 21s 26ms/step - loss: 0.2662 - acc: 0.9065 - val_loss: 0.3475 - val_acc: 0.8897\n",
      "Epoch 79/200\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.2656 - acc: 0.9091 - val_loss: 0.3287 - val_acc: 0.8952\n",
      "Epoch 80/200\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.2549 - acc: 0.9100 - val_loss: 0.3329 - val_acc: 0.8941\n",
      "Epoch 81/200\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.2580 - acc: 0.9114 - val_loss: 0.3316 - val_acc: 0.8928\n",
      "Epoch 82/200\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.2583 - acc: 0.9095 - val_loss: 0.3404 - val_acc: 0.8930\n",
      "Epoch 83/200\n",
      "781/781 [==============================] - 21s 26ms/step - loss: 0.2556 - acc: 0.9111 - val_loss: 0.3521 - val_acc: 0.8851\n",
      "Epoch 84/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.2552 - acc: 0.9089 - val_loss: 0.3173 - val_acc: 0.8990\n",
      "Epoch 85/200\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.2491 - acc: 0.9123 - val_loss: 0.3439 - val_acc: 0.8883\n",
      "Epoch 86/200\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.2492 - acc: 0.9122 - val_loss: 0.3254 - val_acc: 0.8961\n",
      "Epoch 87/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.2463 - acc: 0.9147 - val_loss: 0.3251 - val_acc: 0.8959\n",
      "Epoch 88/200\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.2527 - acc: 0.9109 - val_loss: 0.3236 - val_acc: 0.8952\n",
      "Epoch 89/200\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.2542 - acc: 0.9117 - val_loss: 0.3393 - val_acc: 0.8939\n",
      "Epoch 90/200\n",
      "781/781 [==============================] - 23s 30ms/step - loss: 0.2466 - acc: 0.9119 - val_loss: 0.3276 - val_acc: 0.8962\n",
      "Epoch 91/200\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.2465 - acc: 0.9156 - val_loss: 0.3266 - val_acc: 0.8974\n",
      "Epoch 92/200\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.2479 - acc: 0.9124 - val_loss: 0.3156 - val_acc: 0.8996\n",
      "Epoch 93/200\n",
      "781/781 [==============================] - 21s 26ms/step - loss: 0.2468 - acc: 0.9134 - val_loss: 0.3180 - val_acc: 0.8987\n",
      "Epoch 94/200\n",
      "781/781 [==============================] - 21s 26ms/step - loss: 0.2403 - acc: 0.9170 - val_loss: 0.3316 - val_acc: 0.8950\n",
      "Epoch 95/200\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.2450 - acc: 0.9140 - val_loss: 0.3356 - val_acc: 0.8950\n",
      "Epoch 96/200\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.2400 - acc: 0.9155 - val_loss: 0.3332 - val_acc: 0.8951\n",
      "Epoch 97/200\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.2395 - acc: 0.9158 - val_loss: 0.3261 - val_acc: 0.8966\n",
      "Epoch 98/200\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.2352 - acc: 0.9176 - val_loss: 0.3178 - val_acc: 0.8995\n",
      "Epoch 99/200\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.2385 - acc: 0.9176 - val_loss: 0.3546 - val_acc: 0.8894\n",
      "Epoch 100/200\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.2339 - acc: 0.9181 - val_loss: 0.3415 - val_acc: 0.8927\n",
      "Epoch 101/200\n",
      "781/781 [==============================] - 23s 30ms/step - loss: 0.2403 - acc: 0.9168 - val_loss: 0.3328 - val_acc: 0.8937\n",
      "Epoch 102/200\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.2377 - acc: 0.9167 - val_loss: 0.3178 - val_acc: 0.9011\n",
      "Epoch 00102: early stopping\n",
      "Total training time : 2161.855 s\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.adam(lr=0.001)\n",
    "model.compile(optimizer, keras.losses.categorical_crossentropy, ['accuracy'])\n",
    "\n",
    "shift = 4/32\n",
    "generator = ImageDataGenerator(rotation_range=10, width_shift_range=shift, height_shift_range=shift, \n",
    "                               horizontal_flip=True)\n",
    "\n",
    "batch_size = 64\n",
    "n_steps = x_train.shape[0]//batch_size\n",
    "\n",
    "ckeckpoint = ModelCheckpoint('./Model_trained/model_cifar10.h5', save_best_only=True)\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=25, verbose=1)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit_generator(generator.flow(x_train, y_train, batch_size=batch_size), steps_per_epoch=n_steps, \n",
    "                    epochs=200, validation_data=(x_test, y_test), callbacks=[ckeckpoint, early_stopping])\n",
    "print('Total training time : %.3f s' %(time.time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 197us/step\n",
      "Best model test accuracy : 0.8964\n"
     ]
    }
   ],
   "source": [
    "best_model = keras.models.load_model('./Model_trained/model_cifar10.h5')\n",
    "print('Best model test accuracy :', best_model.evaluate(x_test, y_test, batch_size=64)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

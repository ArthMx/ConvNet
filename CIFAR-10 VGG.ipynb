{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG-like model for the CIFAR-10 dataset\n",
    "\n",
    "In this notebook, I present a model able to reach 90 % accuracy on test set by using data augmentation and using an architecture inspired by the VGG model.\n",
    "\n",
    "- Karen Simonyan, Andrew Zisserman, 2015, Very Deep Convolutional Networks for Large-Scale Image Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthur/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape : (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('X shape :', x_train.shape)\n",
    "print(len(x_train), 'train samples')\n",
    "print(len(x_test), 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_mean = np.mean(x_train, axis=0)\n",
    "x_train_std = np.std(x_train, axis=0)\n",
    "\n",
    "x_train = (x_train - x_train_mean)/x_train_std\n",
    "x_test = (x_test - x_train_mean)/x_train_std\n",
    "\n",
    "n_y = 10\n",
    "y_train = keras.utils.to_categorical(y_train, n_y)\n",
    "y_test = keras.utils.to_categorical(y_test, n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 304,810\n",
      "Trainable params: 304,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "activation = 'relu'\n",
    "l2_reg = keras.regularizers.l2(0.0001)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3,3), padding='same', input_shape=input_shape, kernel_regularizer=l2_reg, \n",
    "                 activation=activation))\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=l2_reg, activation=activation))\n",
    "model.add(MaxPooling2D((2,2), strides=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=l2_reg, activation=activation))\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=l2_reg, activation=activation))\n",
    "model.add(MaxPooling2D((2,2), strides=2))\n",
    "model.add(Dropout(0.25))\n",
    "          \n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=l2_reg, activation=activation))\n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=l2_reg, activation=activation))\n",
    "model.add(AveragePooling2D((8,8)))\n",
    "model.add(Dropout(0.25))\n",
    "          \n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=activation))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(n_y, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "781/781 [==============================] - 18s 22ms/step - loss: 1.9902 - acc: 0.2365 - val_loss: 1.6864 - val_acc: 0.3555\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.35550, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 2/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 1.6788 - acc: 0.3680 - val_loss: 1.6967 - val_acc: 0.3956\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.35550 to 0.39560, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 3/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 1.5278 - acc: 0.4424 - val_loss: 1.3621 - val_acc: 0.5145\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.39560 to 0.51450, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 4/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 1.3799 - acc: 0.5096 - val_loss: 1.2298 - val_acc: 0.5573\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.51450 to 0.55730, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 5/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 1.2739 - acc: 0.5547 - val_loss: 1.2157 - val_acc: 0.5799\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.55730 to 0.57990, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 6/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 1.1898 - acc: 0.5892 - val_loss: 1.0588 - val_acc: 0.6368\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.57990 to 0.63680, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 7/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 1.1202 - acc: 0.6212 - val_loss: 1.1753 - val_acc: 0.6139\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.63680\n",
      "Epoch 8/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 1.0604 - acc: 0.6450 - val_loss: 0.9881 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.63680 to 0.67120, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 9/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 1.0112 - acc: 0.6641 - val_loss: 0.8949 - val_acc: 0.7044\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.67120 to 0.70440, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 10/200\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.9675 - acc: 0.6791 - val_loss: 0.9562 - val_acc: 0.6992\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.70440\n",
      "Epoch 11/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.9263 - acc: 0.6972 - val_loss: 0.8205 - val_acc: 0.7353\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.70440 to 0.73530, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 12/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.8918 - acc: 0.7104 - val_loss: 0.8450 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.73530\n",
      "Epoch 13/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.8601 - acc: 0.7249 - val_loss: 0.8073 - val_acc: 0.7455\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.73530 to 0.74550, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 14/200\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.8332 - acc: 0.7353 - val_loss: 0.7404 - val_acc: 0.7696\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.74550 to 0.76960, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 15/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.8180 - acc: 0.7403 - val_loss: 0.9009 - val_acc: 0.7268\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.76960\n",
      "Epoch 16/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.7987 - acc: 0.7483 - val_loss: 0.6955 - val_acc: 0.7876\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.76960 to 0.78760, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 17/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.7777 - acc: 0.7556 - val_loss: 0.7436 - val_acc: 0.7734\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.78760\n",
      "Epoch 18/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.7668 - acc: 0.7622 - val_loss: 0.6833 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.78760 to 0.78980, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 19/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.7474 - acc: 0.7702 - val_loss: 0.6694 - val_acc: 0.7974\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.78980 to 0.79740, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 20/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.7367 - acc: 0.7749 - val_loss: 0.8259 - val_acc: 0.7521\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.79740\n",
      "Epoch 21/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.7205 - acc: 0.7804 - val_loss: 0.6662 - val_acc: 0.7991\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.79740 to 0.79910, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 22/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.7112 - acc: 0.7840 - val_loss: 0.6926 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.79910\n",
      "Epoch 23/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.7067 - acc: 0.7870 - val_loss: 0.6691 - val_acc: 0.8018\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.79910 to 0.80180, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 24/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.7008 - acc: 0.7887 - val_loss: 0.6650 - val_acc: 0.7989\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.80180\n",
      "Epoch 25/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.6862 - acc: 0.7952 - val_loss: 0.6422 - val_acc: 0.8108\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.80180 to 0.81080, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 26/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.6731 - acc: 0.7981 - val_loss: 0.6722 - val_acc: 0.8034\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.81080\n",
      "Epoch 27/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.6778 - acc: 0.7989 - val_loss: 0.6221 - val_acc: 0.8220\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.81080 to 0.82200, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 28/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.6629 - acc: 0.8030 - val_loss: 0.6604 - val_acc: 0.8108\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.82200\n",
      "Epoch 29/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.6589 - acc: 0.8054 - val_loss: 0.6166 - val_acc: 0.8198\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.82200\n",
      "Epoch 30/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.6509 - acc: 0.8101 - val_loss: 0.7022 - val_acc: 0.8030\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.82200\n",
      "Epoch 31/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.6479 - acc: 0.8122 - val_loss: 0.6253 - val_acc: 0.8186\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.82200\n",
      "Epoch 32/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.6400 - acc: 0.8148 - val_loss: 0.5972 - val_acc: 0.8306\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.82200 to 0.83060, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 33/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.6362 - acc: 0.8134 - val_loss: 0.5870 - val_acc: 0.8341\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.83060 to 0.83410, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 34/200\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6342 - acc: 0.8166 - val_loss: 0.5954 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.83410\n",
      "Epoch 35/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.6188 - acc: 0.8229 - val_loss: 0.6372 - val_acc: 0.8256\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.83410\n",
      "Epoch 36/200\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6239 - acc: 0.8207 - val_loss: 0.6054 - val_acc: 0.8290\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.83410\n",
      "Epoch 37/200\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.6205 - acc: 0.8231 - val_loss: 0.5781 - val_acc: 0.8394\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.83410 to 0.83940, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 38/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.6166 - acc: 0.8240 - val_loss: 0.6018 - val_acc: 0.8331\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.83940\n",
      "Epoch 39/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 16s 21ms/step - loss: 0.6121 - acc: 0.8254 - val_loss: 0.5937 - val_acc: 0.8371\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.83940\n",
      "Epoch 40/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.6052 - acc: 0.8292 - val_loss: 0.5811 - val_acc: 0.8405\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.83940 to 0.84050, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 41/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.6034 - acc: 0.8291 - val_loss: 0.5895 - val_acc: 0.8369\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.84050\n",
      "Epoch 42/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.5959 - acc: 0.8329 - val_loss: 0.5877 - val_acc: 0.8372\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.84050\n",
      "Epoch 43/200\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.5948 - acc: 0.8331 - val_loss: 0.6137 - val_acc: 0.8372\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.84050\n",
      "Epoch 44/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.5940 - acc: 0.8344 - val_loss: 0.5930 - val_acc: 0.8419\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.84050 to 0.84190, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 45/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.5863 - acc: 0.8372 - val_loss: 0.5666 - val_acc: 0.8452\n",
      "\n",
      "Epoch 00045: val_acc improved from 0.84190 to 0.84520, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 46/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.5913 - acc: 0.8349 - val_loss: 0.5936 - val_acc: 0.8406\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.84520\n",
      "Epoch 47/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.5889 - acc: 0.8356 - val_loss: 0.5883 - val_acc: 0.8435\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.84520\n",
      "Epoch 48/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.5841 - acc: 0.8384 - val_loss: 0.6186 - val_acc: 0.8352\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.84520\n",
      "Epoch 49/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.5779 - acc: 0.8403 - val_loss: 0.5902 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00049: val_acc improved from 0.84520 to 0.84810, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 50/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.5833 - acc: 0.8408 - val_loss: 0.5545 - val_acc: 0.8534\n",
      "\n",
      "Epoch 00050: val_acc improved from 0.84810 to 0.85340, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 51/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.5717 - acc: 0.8423 - val_loss: 0.6221 - val_acc: 0.8343\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.85340\n",
      "Epoch 52/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.5780 - acc: 0.8418 - val_loss: 0.6071 - val_acc: 0.8373\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.85340\n",
      "Epoch 53/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.5736 - acc: 0.8438 - val_loss: 0.5610 - val_acc: 0.8535\n",
      "\n",
      "Epoch 00053: val_acc improved from 0.85340 to 0.85350, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 54/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.5691 - acc: 0.8453 - val_loss: 0.5552 - val_acc: 0.8528\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.85350\n",
      "Epoch 55/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.5664 - acc: 0.8458 - val_loss: 0.5692 - val_acc: 0.8513\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.85350\n",
      "Epoch 56/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.5710 - acc: 0.8457 - val_loss: 0.5663 - val_acc: 0.8536\n",
      "\n",
      "Epoch 00056: val_acc improved from 0.85350 to 0.85360, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.0031622775894859655.\n",
      "Epoch 57/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.5101 - acc: 0.8653 - val_loss: 0.5108 - val_acc: 0.8703\n",
      "\n",
      "Epoch 00057: val_acc improved from 0.85360 to 0.87030, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 58/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4962 - acc: 0.8688 - val_loss: 0.5113 - val_acc: 0.8682\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.87030\n",
      "Epoch 59/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4983 - acc: 0.8699 - val_loss: 0.5297 - val_acc: 0.8664\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.87030\n",
      "Epoch 60/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4912 - acc: 0.8710 - val_loss: 0.5289 - val_acc: 0.8669\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.87030\n",
      "Epoch 61/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4881 - acc: 0.8716 - val_loss: 0.5062 - val_acc: 0.8739\n",
      "\n",
      "Epoch 00061: val_acc improved from 0.87030 to 0.87390, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 62/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4794 - acc: 0.8754 - val_loss: 0.5150 - val_acc: 0.8735\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.87390\n",
      "Epoch 63/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4791 - acc: 0.8750 - val_loss: 0.5217 - val_acc: 0.8670\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.87390\n",
      "Epoch 64/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4787 - acc: 0.8755 - val_loss: 0.5173 - val_acc: 0.8699\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.87390\n",
      "Epoch 65/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4784 - acc: 0.8731 - val_loss: 0.5200 - val_acc: 0.8671\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.87390\n",
      "Epoch 66/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4742 - acc: 0.8760 - val_loss: 0.5199 - val_acc: 0.8673\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.87390\n",
      "Epoch 67/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4735 - acc: 0.8756 - val_loss: 0.5153 - val_acc: 0.8689\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.87390\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 0.0009999999903331056.\n",
      "Epoch 68/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4568 - acc: 0.8810 - val_loss: 0.4833 - val_acc: 0.8788\n",
      "\n",
      "Epoch 00068: val_acc improved from 0.87390 to 0.87880, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 69/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4510 - acc: 0.8838 - val_loss: 0.4855 - val_acc: 0.8780\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.87880\n",
      "Epoch 70/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4455 - acc: 0.8836 - val_loss: 0.4901 - val_acc: 0.8770\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.87880\n",
      "Epoch 71/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4447 - acc: 0.8862 - val_loss: 0.5059 - val_acc: 0.8721\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.87880\n",
      "Epoch 72/200\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.4438 - acc: 0.8867 - val_loss: 0.4934 - val_acc: 0.8747\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.87880\n",
      "Epoch 73/200\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.4477 - acc: 0.8838 - val_loss: 0.4977 - val_acc: 0.8759\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.87880\n",
      "Epoch 74/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4461 - acc: 0.8856 - val_loss: 0.5073 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.87880\n",
      "Epoch 75/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4400 - acc: 0.8857 - val_loss: 0.4836 - val_acc: 0.8766\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.87880\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 0.00031622778103685084.\n",
      "Epoch 76/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4347 - acc: 0.8877 - val_loss: 0.4796 - val_acc: 0.8780\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.87880\n",
      "Epoch 77/200\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 0.4313 - acc: 0.8896 - val_loss: 0.4908 - val_acc: 0.8756\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.87880\n",
      "Epoch 78/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4318 - acc: 0.8892 - val_loss: 0.4822 - val_acc: 0.8793\n",
      "\n",
      "Epoch 00078: val_acc improved from 0.87880 to 0.87930, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4323 - acc: 0.8897 - val_loss: 0.4783 - val_acc: 0.8805\n",
      "\n",
      "Epoch 00079: val_acc improved from 0.87930 to 0.88050, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 80/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4314 - acc: 0.8901 - val_loss: 0.4745 - val_acc: 0.8810\n",
      "\n",
      "Epoch 00080: val_acc improved from 0.88050 to 0.88100, saving model to ./Model_trained/VGG_cifar10.h5\n",
      "Epoch 81/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4283 - acc: 0.8898 - val_loss: 0.4799 - val_acc: 0.8803\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.88100\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 0.00010000000639606199.\n",
      "Epoch 82/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4269 - acc: 0.8912 - val_loss: 0.4916 - val_acc: 0.8758\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.88100\n",
      "Epoch 83/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4253 - acc: 0.8914 - val_loss: 0.4863 - val_acc: 0.8769\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.88100\n",
      "Epoch 84/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4284 - acc: 0.8897 - val_loss: 0.4837 - val_acc: 0.8785\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.88100\n",
      "Epoch 85/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4280 - acc: 0.8901 - val_loss: 0.4808 - val_acc: 0.8799\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.88100\n",
      "Epoch 86/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4260 - acc: 0.8908 - val_loss: 0.4852 - val_acc: 0.8771\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.88100\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 3.1622778103685084e-05.\n",
      "Epoch 87/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4257 - acc: 0.8905 - val_loss: 0.4798 - val_acc: 0.8792\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.88100\n",
      "Epoch 88/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4324 - acc: 0.8900 - val_loss: 0.4805 - val_acc: 0.8779\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.88100\n",
      "Epoch 89/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4243 - acc: 0.8928 - val_loss: 0.4807 - val_acc: 0.8789\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.88100\n",
      "Epoch 90/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4264 - acc: 0.8915 - val_loss: 0.4800 - val_acc: 0.8790\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.88100\n",
      "Epoch 91/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4232 - acc: 0.8914 - val_loss: 0.4807 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.88100\n",
      "Epoch 92/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4260 - acc: 0.8912 - val_loss: 0.4837 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.88100\n",
      "Epoch 93/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4268 - acc: 0.8904 - val_loss: 0.4818 - val_acc: 0.8790\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.88100\n",
      "Epoch 94/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4299 - acc: 0.8915 - val_loss: 0.4787 - val_acc: 0.8790\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.88100\n",
      "Epoch 95/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4242 - acc: 0.8920 - val_loss: 0.4809 - val_acc: 0.8786\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.88100\n",
      "Epoch 96/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4248 - acc: 0.8941 - val_loss: 0.4842 - val_acc: 0.8784\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.88100\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 1.0000000409520217e-05.\n",
      "Epoch 97/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4223 - acc: 0.8919 - val_loss: 0.4817 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.88100\n",
      "Epoch 98/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4246 - acc: 0.8916 - val_loss: 0.4822 - val_acc: 0.8788\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.88100\n",
      "Epoch 99/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4242 - acc: 0.8926 - val_loss: 0.4826 - val_acc: 0.8785\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.88100\n",
      "Epoch 100/200\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4226 - acc: 0.8928 - val_loss: 0.4828 - val_acc: 0.8782\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.88100\n",
      "Epoch 00100: early stopping\n",
      "Total training time : 1623.210 s\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.SGD(0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer, keras.losses.categorical_crossentropy, ['accuracy'])\n",
    "\n",
    "shift = 4/32\n",
    "generator = ImageDataGenerator(width_shift_range=shift, height_shift_range=shift, \n",
    "                               horizontal_flip=True)\n",
    "\n",
    "batch_size = 64\n",
    "n_steps = x_train.shape[0]//batch_size\n",
    "\n",
    "save_path = './Model_trained/VGG_cifar10.h5'\n",
    "ckeckpoint = ModelCheckpoint(save_path, monitor='val_acc', verbose=1, save_best_only=True)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=np.sqrt(0.1), patience=5, \n",
    "                                              min_delta=0.01, min_lr=1e-6, verbose=1)\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=20, verbose=1)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit_generator(generator.flow(x_train, y_train, batch_size=batch_size), steps_per_epoch=n_steps, \n",
    "                    epochs=200, validation_data=(x_test, y_test), callbacks=[ckeckpoint, reduce_lr, early_stopping])\n",
    "print('Total training time : %.3f s' %(time.time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 102us/step\n",
      "Best model test accuracy : 0.881\n"
     ]
    }
   ],
   "source": [
    "best_model = keras.models.load_model(save_path)\n",
    "print('Best model test accuracy :', best_model.evaluate(x_test, y_test, batch_size=64)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Building a deep ConvNet, inspired by the VGG-Net, and using dropout, let us reach $88.1 \\%$ of accuracy on the CIFAR-10 dataset. The use of dropout let us restrict the overfitting of the model, despite his high number of parameters.\n",
    "\n",
    "The closeness between the training and testing loss and the low training accuracy, let us think that increasing a bit more the depth would lead to increased accuracy results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
